# scanning/ Stub Detection

How scanning/ identifies stub functions (incomplete implementations) and computes the `stub_ratio` and `impl_gini` signals. These signals are central to the AI wiring quality score -- codebases generated by AI assistants often have high stub ratios and bimodal implementation distributions.

---

## Signals Computed

This document describes **how** to compute two signals. Their formulas are the source of truth in `registry/signals.md`:

- **Signal #5 `impl_gini`** -- see `registry/signals.md #5`. Gini coefficient of function body token counts within a file. Measures implementation evenness.
- **Signal #6 `stub_ratio`** -- see `registry/signals.md #6`. Mean stub score across all functions in a file. Measures incompleteness.

---

## Stub Score Per Function

For each `FunctionDef` in a file, compute a continuous stub score in [0, 1]:

```
stub_score(f) = 1 - min(1, body_tokens(f) / (signature_tokens(f) * 3))
```

Where:
- `body_tokens(f)` = token count in the function body (from `FunctionDef.body_tokens`)
- `signature_tokens(f)` = token count in the function signature line (from `FunctionDef.signature_tokens`)
- The multiplier `3` is a scaling factor: a function with 8 signature tokens should have at least 24 body tokens to score 0.0 (not a stub)

**Interpretation:**
- `stub_score = 0.0` -- body is at least 3x the signature in tokens (fully implemented)
- `stub_score = 0.5` -- body is 1.5x the signature (partial implementation)
- `stub_score = 1.0` -- body has no tokens beyond the signature (pure stub)

---

## Hard Classification Patterns

Before computing the continuous stub score, check for hard-classified stubs. These are functions whose body is a known stub pattern, regardless of token count:

```
is_hard_stub(f) =
    body_tokens(f) < 5
    OR body_source(f) matches any of:
        ^\s*pass\s*$
        ^\s*\.\.\.\s*$
        ^\s*return\s+None\s*$
        ^\s*return\s*$
        ^\s*raise\s+NotImplementedError.*$
        ^\s*throw\s+.*NotImplemented.*$
        ^\s*todo!.*$                          # Rust
        ^\s*unimplemented!.*$                 # Rust
        ^\s*panic!\(.*\)\s*$                  # Rust
```

Hard-classified stubs always get `stub_score = 1.0`, overriding the continuous formula.

### Per-Language Stub Patterns

| Language | Stub body patterns |
|----------|--------------------|
| Python | `pass`, `...`, `return None`, `return`, `raise NotImplementedError(...)` |
| Go | `panic("not implemented")`, `return` with zero-value, empty function body `{}` |
| TypeScript/JS | `throw new Error("Not implemented")`, `return undefined`, empty body `{}` |
| Java | `throw new UnsupportedOperationException(...)`, `return null`, `return 0`, empty body `{}` |
| Rust | `todo!()`, `unimplemented!()`, `panic!(...)` |
| Ruby | `raise NotImplementedError`, `# TODO`, empty method `end` immediately after `def` |
| C/C++ | `return 0;` as sole statement, empty body `{}`, `assert(false)` |

These patterns are checked by the normalizer after tree-sitter extraction. The body source text is available from `FunctionDef.body_source`.

---

## File-Level Stub Ratio

```
stub_ratio(file) = mean(stub_score(f) for f in file.functions)
```

If `file.functions` is empty, `stub_ratio = 0.0` (no functions = no stubs).

**Threshold** (from `registry/signals.md #6`): `stub_ratio > 0.5` is flagged. This means more than half the function implementation weight is stubs.

---

## Implementation Gini (impl_gini)

Measures how evenly functions are implemented within a file. Computed from the distribution of body token counts.

### Input

```
sizes = [f.body_tokens for f in file.functions]
```

If `len(sizes) < 2`, `impl_gini = 0.0` (need at least 2 functions for a meaningful distribution).

### Computation

Standard Gini coefficient:

```
sorted_sizes = sorted(sizes)
n = len(sorted_sizes)
numerator = sum((2*i - n - 1) * sorted_sizes[i] for i in range(n))
denominator = n * sum(sorted_sizes)
gini = numerator / denominator if denominator > 0 else 0.0
```

Alternatively, using the mean-difference formula:

```
gini = sum(abs(x_i - x_j) for all i,j pairs) / (2 * n * mean(sizes))
```

### Interpretation

- `impl_gini ~ 0.0` -- all functions have similar body sizes (uniform implementation)
- `impl_gini ~ 0.3` -- moderate variation (normal for most files)
- `impl_gini > 0.6` -- bimodal distribution: some functions are fully implemented, others are stubs or trivial. This is the **AI-generated code signature** -- AI assistants often implement a few key functions and leave others as stubs.

**Threshold** (from `registry/signals.md #5`): `impl_gini > 0.6` is flagged.

---

## Function Size Entropy

An alternative view of implementation distribution, useful for detecting god functions:

```
total = sum(body_tokens for all functions)
p_i = body_tokens(f_i) / total          for each function f_i
H = -sum(p_i * log2(p_i))              Shannon entropy of size distribution
H_max = log2(len(functions))            maximum possible entropy
evenness = H / H_max                    normalized evenness [0, 1]
```

- `evenness ~ 1.0` -- all functions contribute equally (well-decomposed)
- `evenness < 0.3` -- one or two functions contain almost all the logic (god function risk)

Function size entropy is not a registered signal in v2 but is available as diagnostic output for the `explain` command. It may be promoted to a signal in a future version.

---

## Interaction with Downstream Modules

### semantics/ (IR2)

semantics/ receives `stub_ratio` and `impl_gini` as part of its `Completeness` assessment. Combined with `docstring_coverage` and `todo_density`, these form the completeness profile of a file.

### signals/ (IR5s)

signals/ uses `stub_ratio` in:
- Signal #36 `wiring_quality` (see `registry/signals.md #36`) -- `0.25 * stub_ratio` weight
- Signal #60 `wiring_score` (global) -- `0.15 * mean(stub_ratio)` weight
- Signal #51 `health_score` (module) -- `0.15 * (1 - mean_stub_ratio)` weight

### insights/ (IR6)

Finders that consume stub signals:
- **GodFile** -- high function count + high impl_gini = some functions over-implemented, others empty
- **AI quality assessment** -- high stub_ratio + high impl_gini = likely AI-scaffolded code

---

## What Exists Today vs What's New

### EXISTS TODAY (v1)

- `FileMetrics.function_sizes` provides a list of line counts per function (not token counts)
- `impl_gini` is computed from line counts in `signals/plugins/coherence.py` (named `implementation_gini`)
- No per-function stub detection
- No `stub_ratio` signal
- No hard classification patterns
- No `body_tokens` or `signature_tokens` fields

### NEW IN v2

- Per-function `FunctionDef` with `body_tokens` and `signature_tokens` enables token-based stub scoring (more accurate than line counts)
- Continuous `stub_score` per function plus hard classification patterns
- File-level `stub_ratio` as a registered signal (#6)
- `impl_gini` recomputed from token counts instead of line counts (more language-agnostic)
- Per-language stub body patterns for hard classification
- Function size entropy as diagnostic output

### Implementation location

All stub detection logic lives in `scanning/stubs.py`:

```python
def stub_score(func: FunctionDef) -> float:
    """Continuous stub score for a single function."""
    ...

def is_hard_stub(func: FunctionDef, language: Language) -> bool:
    """Hard classification against known stub patterns."""
    ...

def file_stub_ratio(functions: list[FunctionDef], language: Language) -> float:
    """Mean stub score across all functions in a file."""
    ...

def file_impl_gini(functions: list[FunctionDef]) -> float:
    """Gini coefficient of body_tokens distribution."""
    ...
```

These functions are called by the normalizer during the scanning pass, and the results are stored on `FileSyntax` or returned alongside it as computed signals.
